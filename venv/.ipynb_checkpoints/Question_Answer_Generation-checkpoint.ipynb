{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f193939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-02-17 10:47:13 +05:30)\n"
     ]
    }
   ],
   "source": [
    "#!pip install --quiet ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9720c865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-02-17 10:48:36 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf913c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Entry Not Found for url: https://huggingface.co/sshleifer/distilbart-cnn-6-6/resolve/main/tf_model.h5\n",
      "404 Client Error: Entry Not Found for url: https://huggingface.co/sshleifer/distilbart-cnn-6-6/resolve/main/tf_model.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model sshleifer/distilbart-cnn-6-6 with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\NACHIK~1.B\\AppData\\Local\\Temp/ipykernel_6368/1093389107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mto_tokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"In a traditional world, most companies resolve support issues manually. In this approach, customers or employees request a ticket through e-mail, ITSM portal, or call. This ticket is picked by a Level 1 technician based on availability after a long wait. During this time, customer or employee is frustrated and often unproductive waiting for their issues to be resolved. If the Level 1 technician can not resolve the issue, it will be escalated to Level 2, and the cycle of wait time and explanation restarts, increasing loss in productivity and frustration. If the Level 2 technical cannot resolve, it goes to Level 3 and then to the Supervisor / Manager. This approach gives the most human touch possible. It is not productive and efficient in this current world where time is gold, and no one wants to wait even for a few minutes, and surprisingly, customers and employees have to wait for minutes, hours, or in few cases - even days.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msummarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"summarization\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sshleifer/distilbart-cnn-6-6\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sshleifer/distilbart-cnn-6-6\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0msummarized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;31m# Will load the correct model if possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[0mmodel_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m     framework, model = infer_framework_load_model(\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mmodel_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Could not load model {model} with any of the following classes: {class_tuple}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mframework\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tf\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TF\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model sshleifer/distilbart-cnn-6-6 with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration'>)."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.7 s (started: 2022-02-17 11:16:19 +05:30)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-6-6\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-6-6\")\n",
    "\n",
    "to_tokenize = \"\"\"In a traditional world, most companies resolve support issues manually. In this approach, customers or employees request a ticket through e-mail, ITSM portal, or call. This ticket is picked by a Level 1 technician based on availability after a long wait. During this time, customer or employee is frustrated and often unproductive waiting for their issues to be resolved. If the Level 1 technician can not resolve the issue, it will be escalated to Level 2, and the cycle of wait time and explanation restarts, increasing loss in productivity and frustration. If the Level 2 technical cannot resolve, it goes to Level 3 and then to the Supervisor / Manager. This approach gives the most human touch possible. It is not productive and efficient in this current world where time is gold, and no one wants to wait even for a few minutes, and surprisingly, customers and employees have to wait for minutes, hours, or in few cases - even days.\"\"\"\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer,framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093bfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd64af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' In a traditional world, most companies resolve support issues manually . In this approach, customers or employees request a ticket through e-mail, ITSM portal, or call . This ticket is picked by a Level 1 technician based on availability after a long wait . It is not productive and efficient in this current world where time is gold, and no one wants to wait even for a'}]\n",
      "time: 0 ns (started: 2022-02-14 12:10:01 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# Print summarized text\n",
    "print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "747a8b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-02-14 12:18:06 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "#keyword extraction\n",
    "def multipartite_keyword(text):\n",
    "    out=[]\n",
    "    try:\n",
    "        extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "        # 2. load the content of the document.\n",
    "        extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "\n",
    "        # 3. select {1-3}-grams not containing punctuation marks and not\n",
    "        #    beginning/ending with a stopword as candidates.\n",
    "        stoplist = stopwords.words('english')\n",
    "        extractor.candidate_selection(n=3, stoplist=stoplist)\n",
    "\n",
    "        # 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "        #    words) for computing left/right contexts can be specified.\n",
    "        window = 2\n",
    "        use_stems = False # use stems instead of words for weighting\n",
    "        extractor.candidate_weighting(window=window,\n",
    "                              stoplist=stoplist,\n",
    "                              use_stems=use_stems)\n",
    "\n",
    "        # 5. get the 10-highest scored candidates as keyphrases.\n",
    "        #    redundant keyphrases are removed from the output using levenshtein\n",
    "        #    distance and a threshold.\n",
    "        threshold = 0.8\n",
    "        keyphrases = extractor.get_n_best(n=15, threshold=threshold)\n",
    "\n",
    "        for val in keyphrases:\n",
    "            out.append(val[0])\n",
    "    except:\n",
    "        out = []\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c1929cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 156 ms (started: 2022-02-14 12:18:21 +05:30)\n"
     ]
    }
   ],
   "source": [
    "#sumary text sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "summary=summarized[0]['summary_text']\n",
    "summary_tokenize=sent_tokenize(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ece28cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2022-02-14 12:18:22 +05:30)\n"
     ]
    }
   ],
   "source": [
    "#keyword mapping\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "\n",
    "def get_keywords(originaltext,summarytext):\n",
    "    keywords =  multipartite_keyword(originaltext)\n",
    "    print (\"keywords unsummarized: \",keywords)\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    for keyword in keywords:\n",
    "        keyword_processor.add_keyword(keyword)\n",
    "\n",
    "    keywords_found = keyword_processor.extract_keywords(summarytext)\n",
    "    keywords_found = list(set(keywords_found))\n",
    "    print (\"keywords_found in summarized: \",keywords_found)\n",
    "\n",
    "    important_keywords =[]\n",
    "    for keyword in keywords:\n",
    "        if keyword in keywords_found:\n",
    "            important_keywords.append(keyword)\n",
    "\n",
    "    return important_keywords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a49c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords unsummarized:  ['support issues manually', 'companies resolve support', 'level', 'resolve support issues', 'issues manually', 'itsm portal', 'support issues', 'companies resolve', 'resolve support', 'traditional world', 'wait', 'resolve', 'manually', 'time', 'traditional']\n",
      "keywords_found in summarized:  ['companies resolve support', 'traditional world', 'wait', 'level', 'itsm portal', 'time', 'issues manually']\n",
      "['companies resolve support', 'level', 'issues manually', 'itsm portal', 'traditional world', 'wait', 'time']\n",
      "time: 641 ms (started: 2022-02-14 12:18:22 +05:30)\n"
     ]
    }
   ],
   "source": [
    "imp_keywords = get_keywords(to_tokenize,summary)\n",
    "print (imp_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6440c39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.1 s (started: 2022-02-14 12:18:23 +05:30)\n"
     ]
    }
   ],
   "source": [
    "#called pretrained question generator model\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
    "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
    "question_model = question_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e9cf96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-02-14 12:44:46 +05:30)\n"
     ]
    }
   ],
   "source": [
    "def get_question(context,answer,model,tokenizer):\n",
    "    text = \"context: {} answer: {}\".format(context,answer)\n",
    "    encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
    "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "    outs = model.generate(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  early_stopping=True,\n",
    "                                  num_beams=5,\n",
    "                                  num_return_sequences=1,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  max_length=72)\n",
    "\n",
    "\n",
    "    dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
    "\n",
    "\n",
    "    Question = dec[0].replace(\"question:\",\"\")\n",
    "    Question= Question.strip()\n",
    "    return Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5061b4b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do most companies handle support issues?\n",
      " in a traditional world, most companies resolve support issues manually .\n",
      "\n",
      "\n",
      "How do most companies resolve support issues manually?\n",
      "In this approach, customers or employees request a ticket through e-mail, itsm portal, or call .\n",
      "\n",
      "\n",
      "How is a ticket picked?\n",
      "This ticket is picked by a level 1 technician based on availability after a long wait .\n",
      "\n",
      "\n",
      "Is it productive and efficient in a world where time is gold?\n",
      "It is not productive and efficient in this current world where time is gold, and no one wants to wait even for a\n",
      "\n",
      "\n",
      "time: 10.3 s (started: 2022-02-14 12:44:47 +05:30)\n"
     ]
    }
   ],
   "source": [
    "ls=[]\n",
    "for answer in summary_tokenize:\n",
    "    ques = get_question(summary,answer,question_model,question_tokenizer)\n",
    "    q=ques\n",
    "    a=answer.capitalize\n",
    "    b=[q,a]\n",
    "    ls.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ab769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4ff18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
